{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":928025,"sourceType":"datasetVersion","datasetId":500970}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q pandas\n!pip install -q librosa\n!pip install -q plotly\n!pip install -q matplotlib\n!pip install -q mutagen\n!pip install -q pillow\n!pip install -q resampy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 📚 Kütüphanleri Yükleyelim (Import Libs)","metadata":{}},{"cell_type":"code","source":"# Data manipulation and processing\nimport pandas as pd\nimport numpy as np\nimport random\n\nfrom PIL import Image\n\nimport os\nimport time\nimport librosa\nimport librosa.display\n\n# import mutagen\n# import mutagen.wave\n\nfrom tqdm import tqdm, trange\nfrom tqdm.auto import tqdm\n\nimport IPython.display as ipd\nimport IPython.display\n\n# Visualization libraries\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# To suppress warnings\nimport warnings \nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T17:50:15.902350Z","iopub.execute_input":"2025-02-23T17:50:15.902732Z","iopub.status.idle":"2025-02-23T17:50:18.604179Z","shell.execute_reply.started":"2025-02-23T17:50:15.902687Z","shell.execute_reply":"2025-02-23T17:50:18.602887Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\n\n# Data Split and Cross-Validation for Image Data\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold  \n\n# Evaluation Metrics for Image Classification\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, \n    precision_score, recall_score, accuracy_score, \n    roc_curve, auc, f1_score, log_loss  \n)\n\n# Label Binarization for Multi-Class Image Classification\nfrom sklearn.preprocessing import label_binarize  # Used for multi-class classification\n\n# Class Weights Calculation for Imbalanced Image Datasets\nfrom sklearn.utils.class_weight import compute_class_weight  # Used for handling class imbalance in images\n\n# Model Selection Metrics for Image Classification\nfrom sklearn.metrics import roc_auc_score, matthews_corrcoef  # Added ROC AUC score and Matthews correlation coefficient\n\n# Additional Image Classification Metrics\nfrom sklearn.metrics import average_precision_score, precision_recall_curve  # Added precision-recall curve and average precision score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 📖 Veriyi anlamaya çalışalım (Understanding the data)","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/urbansound8k/UrbanSound8K.csv\")\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# tüm dosya yolunu almak ve oluşturmak için\ndf[\"file_path\"] = df.apply(lambda row: f'/kaggle/input/urbansound8k/fold{row[\"fold\"]}/{row[\"slice_file_name\"]}', axis=1)\n\n# süreyi hesaplayalım öylesine çok sormayın :p\ndf[\"time\"] = df[\"end\"] - df[\"start\"]\n\n# Gereksiz sütunları kaldırıyorum\ndf = df.drop(columns=[\"start\", \"end\", \"fold\", \"slice_file_name\", \"fsID\", \"classID\"], axis=1)\n\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['salience'].value_counts() # salince: sesin belirginliğini belirtir!","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['class'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"count = df['class'].value_counts()\n\nfig, axs = plt.subplots(1, 2, figsize = (12,6), facecolor = 'white')\n\npalette = sns.color_palette(\"Paired\")\nsns.set_palette(palette)\naxs[0].pie(count, labels = count.index, autopct = '%1.1f%%', startangle = 140)\naxs[0].set_title('Distribution of categories')\n\nsns.barplot(x=count.index, y=count.values, ax = axs[1], palette = 'Paired')\naxs[1].set_title('Count of each Category')\naxs[1].tick_params(axis='x', rotation=45) \n\nfor i, val in enumerate(count.values):\n    axs[1].text(i, val, str(val), ha='center', va='bottom')\n    \nplt.tight_layout()\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.hist(df['time'], bins=30, color='skyblue', edgecolor='black')\nplt.title(\"Distribution of Audio Slice Durations\")\nplt.xlabel(\"Duration (seconds)\")\nplt.ylabel(\"Frequency\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_spectrogram(audio_path:str, figsize:tuple=(20,20), cmap='magma',y_axis:str='log'):\n    \"\"\"\n    Ses dosyasının Spectrogramını gösteren fonksiyon.\n    :param audio_path: Ses dosyasının yolu (str)\n    :param figsize: Görselin boyutu (tuple)\n    :param cmap: Görselin renk haritası (str)\n    :param y_axis: Y eksenin türü ('log', 'linear', 'mel')\n    \"\"\"\n    \n    # ses dosyasını yüklüyoruz\n    try:\n        y, sr = librosa.load(audio_path, sr=None)\n    except Exception as e:\n        print(e)\n        return \n    \n    # Short-Time Fourier Transform (STFT) ile frekans analizi yapıyoruz\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n\n    # Spectrogram'ı çizdirelim\n    plt.figure(figsize=figsize)\n    librosa.display.specshow(D, sr=sr, y_axis=y_axis, x_axis='time', cmap=cmap) # plasma\n    plt.colorbar(format='%+2.0f dB')\n    plt.title('Spectrogram')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_spectrogram('/kaggle/input/urbansound8k/fold4/102102-3-0-0.wav',\n                figsize=(10,5), cmap=\"plasma\")\n\nIPython.display.Audio('/kaggle/input/urbansound8k/fold4/102102-3-0-0.wav')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_spectrogram(audio_path='../input/urbansound8k/fold5/100263-2-0-117.wav',\n                 figsize=(10,5),cmap='magma')\n\nIPython.display.Audio('../input/urbansound8k/fold5/100263-2-0-117.wav')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_spectrogram(audio_path='../input/urbansound8k/fold5/100032-3-0-0.wav',\n                 figsize=(10,5))\n\nIPython.display.Audio('../input/urbansound8k/fold5/100032-3-0-0.wav')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_samples = df.groupby('class').sample(1)\naudio_samples, labels = random_samples['file_path'].tolist(), random_samples['class'].tolist()\n\n# Visualize the waveforms\nfig, axs = plt.subplots(5, 2, figsize=(15,15))\nindex = 0\n\nfor col in range(2):\n    for row in range(5):\n        audio_file, sample_rate = librosa.load(audio_samples[index])\n        # Use waveshow instead of waveplot\n        librosa.display.waveshow(audio_file, sr=sample_rate, ax=axs[row][col])\n        axs[row][col].set_title('{}'.format(labels[index]))\n        index += 1\n\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 15 random dosya seçiyoruz\nrandom_samples = df.sample(15)\n\n# 3x5 matris oluşturuyoruz\nfig, axes = plt.subplots(3, 5, figsize=(20, 12))\n\nfor i, (_, row) in enumerate(random_samples.iterrows()):\n    \n    file_path = row[\"file_path\"]\n    label = row[\"class\"] \n\n    row = i // 5\n    col = i % 5\n\n    # Ses dosyasını yükle\n    y, sr = librosa.load(file_path, sr=None)\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n\n    # Spectrogram çizdir\n    librosa.display.specshow(D, sr=sr, y_axis=\"log\", x_axis=\"time\", cmap=\"magma\", ax=axes[row, col])\n    axes[row, col].set_title(label)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 🎶 Veri seti oluşturalım (Creating dataset)","metadata":{}},{"cell_type":"code","source":"def extract_features(file_name:str):\n\n    try:\n        y, sr = librosa.load(file_name, sr = None)\n\n        if len(y) < sr * 0.1:  # 100 ms'den kısa\n            print(f\"Çok kısa ses dosyası: {file_name}, süre: {len(y)/sr:.3f}s\")\n        \n        \n        # MFCC (Mel-Frequency Cepstral Coefficients)\n        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=1024)\n        mfcc_mean = np.mean(mfcc.T, axis=0)\n    \n        # Chroma Features\n        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n        chroma_mean = np.mean(chroma.T, axis=0)\n    \n        # Spectral Contrast\n        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n        spectral_contrast_mean = np.mean(spectral_contrast.T, axis=0)\n    \n        # Zero-Crossing Rate\n        zcr = librosa.feature.zero_crossing_rate(y)\n        zcr_mean = np.mean(zcr)\n    \n        # Spectral Centroid\n        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n        spectral_centroid_mean = np.mean(spectral_centroid)\n    \n        return mfcc_mean, chroma_mean, spectral_contrast_mean, zcr_mean, spectral_centroid_mean\n        \n    except Exception as e:\n        print(f\"Hata oluştu: {e}\")\n        return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_file(row):\n    try:\n        file_name = row['file_path']\n        features = extract_features(file_name)\n        \n        if features is not None:\n            mfcc, chroma, spectral_contrast, zcr, spectral_centroid = features\n            return [*mfcc, *chroma, *spectral_contrast, zcr, spectral_centroid, row[\"class\"]]\n    except Exception as e:\n        print(f\"⚠️ İşleme hatası: {file_name} - {e}\")\n        return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom joblib import Parallel, delayed\n\ndef extract_features_parallel(df, n_processes=None):\n    if n_processes is None:\n        n_processes = os.cpu_count() - 1  # CPU sayısını otomatik al\n\n    print(f\"{n_processes} işlemci kullanılıyor\")\n\n    # results = Parallel(n_jobs=n_processes)(delayed(process_file)(row) for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Özellikler Çıkarılıyor\"))\n\n    with Parallel(n_jobs=n_processes, backend='loky') as parallel: # 'threading' veya 'multiprocessing' \n        results = parallel(delayed(process_file)(row) for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Özellikler Çıkarılıyor\"))\n\n    \n    # None dönen sonuçları filtrele\n    results = [r for r in results if r is not None]\n\n    columns = [f'MFCC_{i+1}' for i in range(13)] + \\\n              [f'Chroma_{i+1}' for i in range(12)] + \\\n              [f'SpectralContrast_{i+1}' for i in range(7)] + \\\n              ['ZeroCrossingRate', 'SpectralCentroid', 'class']\n\n    return pd.DataFrame(results, columns=columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Kullanım\next_df = extract_features_parallel(df)\nprint(ext_df.head())\n\n# CSV'ye kaydetme\next_df.to_csv('extracted_audio_features.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### parallel işleme olmadan","metadata":{}},{"cell_type":"code","source":"extracted = []\nfor index_num, row in tqdm(df.iterrows(), total=len(df)):\n    file_name = row['file_path']\n    \n    features = extract_features(file_name)\n    if features is not None:\n        mfcc, chroma, spectral_contrast, zcr, spectral_centroid = features\n        extracted.append([*mfcc, *chroma, *spectral_contrast, zcr, spectral_centroid, row[\"class\"]])\n    else:\n        print(f\"⚠️ Skipping file due to extraction error: {file_name}\")\n\ncolumns = [f'MFCC_{i+1}' for i in range(13)] + \\\n          [f'Chroma_{i+1}' for i in range(12)] + \\\n          [f'SpectralContrast_{i+1}' for i in range(7)] + \\\n          ['ZeroCrossingRate', 'SpectralCentroid', 'class']\n\next_df = pd.DataFrame(extracted, columns=columns)\n\nprint(ext_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### veriyi ayıralım ","metadata":{}},{"cell_type":"code","source":"ext_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = ext_df.drop(columns=['class'])  \ny = ext_df['class']\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"X shape:\", X.shape)\nprint(\"y_encoded shape:\", len(y_encoded))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_temp, y_train, y_temp = train_test_split(X, y_encoded, test_size=0.2999, random_state=42, stratify = y_encoded)\nX_test, X_valid, y_test, y_valid = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42, stratify = y_temp)\n\ntrain_percentage = (X_train.shape[0] / X.shape[0]) * 100\ntest_percentage = (X_test.shape[0] / X.shape[0]) * 100\nvalid_percentage = (X_valid.shape[0] / X.shape[0]) * 100\n\nprint(\"Training set shapes:\", X_train.shape, f\"({train_percentage:.2f}%)\")\nprint(\"Testing set shapes:\", X_test.shape, f\"({test_percentage:.2f}%)\")\nprint(\"Validation set shapes:\", X_valid.shape, f\"({valid_percentage:.2f}%)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modelleme (Modelling)","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier, AdaBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\nimport optuna","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LogisticRegression","metadata":{}},{"cell_type":"code","source":"log_reg = LogisticRegression(random_state=42, max_iter=1000) \nlog_reg.fit(X_train, y_train)\n\ny_valid_pred = log_reg.predict(X_valid)\n\nvalid_accuracy = accuracy_score(y_valid, y_valid_pred)\nprint(\"\\nValidation Accuracy:\", valid_accuracy)\nprint(\"\\nClassification Report on Validation Set:\")\nprint(classification_report(y_valid, y_valid_pred))\n\ny_test_pred = log_reg.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint(\"\\nTest Accuracy:\", test_accuracy)\nprint(\"\\nClassification Report on Test Set:\")\nprint(classification_report(y_test, y_test_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial):\n    # Define hyperparameter search space\n    C = trial.suggest_float('C', 1e-4, 1e2, log=True)  # Regularization strength\n    solver = trial.suggest_categorical('solver', ['lbfgs', 'liblinear', 'saga'])\n    \n    # Additional parameters based on solver compatibility\n    if solver in ['lbfgs', 'saga']:\n        max_iter = trial.suggest_int('max_iter', 100, 1000)\n    else:\n        max_iter = 1000  # Default for liblinear\n\n    # Initialize and train the model\n    log_reg = LogisticRegression(\n        C=C,\n        solver=solver,\n        max_iter=max_iter,\n        random_state=42\n    )\n    \n    log_reg.fit(X_train, y_train)\n    \n    # Make predictions on validation set\n    y_valid_pred = log_reg.predict(X_valid)\n    \n    # Calculate accuracy\n    valid_accuracy = accuracy_score(y_valid, y_valid_pred)\n    \n    return valid_accuracy\n\n# Create and run the Optuna study\nstudy = optuna.create_study(direction='maximize')  # Maximize accuracy\nstudy.optimize(objective, n_trials=50)  # Run 50 trials\n\n# Print the best parameters and score\nprint(\"\\nBest parameters:\", study.best_params)\nprint(\"Best validation accuracy:\", study.best_value)\n\n# Train the final model with the best parameters\nbest_params = study.best_params\nfinal_model = LogisticRegression(\n    C=best_params['C'],\n    solver=best_params['solver'],\n    max_iter=best_params.get('max_iter', 1000),  # Default if not in best_params\n    random_state=42\n)\nfinal_model.fit(X_train, y_train)\n\n# Evaluate on validation set\ny_valid_pred = final_model.predict(X_valid)\nvalid_accuracy = accuracy_score(y_valid, y_valid_pred)\nprint(\"\\nFinal Validation Accuracy:\", valid_accuracy)\nprint(\"\\nClassification Report on Validation Set:\")\nprint(classification_report(y_valid, y_valid_pred))\n\n# Evaluate on test set\ny_test_pred = final_model.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint(\"\\nFinal Test Accuracy:\", test_accuracy)\nprint(\"\\nClassification Report on Test Set:\")\nprint(classification_report(y_test, y_test_pred))\n\n# Optional: Visualize the optimization process\noptuna.visualization.plot_optimization_history(study).show()\noptuna.visualization.plot_param_importances(study).show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## XGBClassifier","metadata":{}},{"cell_type":"code","source":"xgb_model = XGBClassifier(random_state=42, use_label_encoder=True)\nxgb_model.fit(X_train, y_train)\nxgb_predictions = xgb_model.predict(X_valid)\n\nxgb_accuracy = accuracy_score(y_valid, xgb_predictions)\nprint(\"XGBoost Accuracy:\", xgb_accuracy)\n\n# Compute confusion matrix\ncm = confusion_matrix(y_valid, xgb_predictions)\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()\n\n# Print classification report\nreport = classification_report(y_valid, xgb_predictions)\nprint(\"Classification Report:\")\nprint(report)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial):\n    # Define hyperparameter search space\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'gamma': trial.suggest_float('gamma', 0, 5),\n        'random_state': 42\n    }\n    \n    model = XGBClassifier(**params, use_label_encoder=False)\n    model.fit(X_train, y_train)\n    \n    predictions = model.predict(X_valid)\n    accuracy = accuracy_score(y_valid, predictions)\n    \n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)  # You can adjust n_trials\n\nprint(\"Best parameters:\", study.best_params)\nprint(\"Best accuracy:\", study.best_value)\n\nbest_params = study.best_params\nbest_model = XGBClassifier(**best_params, use_label_encoder=False, random_state=42)\nbest_model.fit(X_train, y_train)\n\nxgb_predictions = best_model.predict(X_valid)\n\nxgb_accuracy = accuracy_score(y_valid, xgb_predictions)\nprint(\"\\nXGBoost Accuracy with Optimized Parameters:\", xgb_accuracy)\n\ncm = confusion_matrix(y_valid, xgb_predictions)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix - Optimized XGBoost\")\nplt.show()\n\nreport = classification_report(y_valid, xgb_predictions)\nprint(\"\\nClassification Report:\")\nprint(report)\n\nfeature_importance = best_model.feature_importances_\nfor i, importance in enumerate(feature_importance):\n    print(f\"Feature {i}: {importance}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CatBoostClassifier","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    # Define hyperparameter search space for CatBoost\n    params = {\n        'depth': trial.suggest_int('depth', 4, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'iterations': trial.suggest_int('iterations', 100, 1000),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n        'random_strength': trial.suggest_float('random_strength', 0, 10),\n        'border_count': trial.suggest_int('border_count', 32, 255),\n        'random_seed': 42,\n        'verbose': 0  # Suppress CatBoost output\n    }\n    \n    model = CatBoostClassifier(**params)\n    model.fit(X_train, y_train)\n    \n    predictions = model.predict(X_valid)\n    accuracy = accuracy_score(y_valid, predictions)\n    \n    return accuracy\n\n# Create and run the optimization study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)  # You can adjust n_trials\n\nprint(\"Best parameters:\", study.best_params)\nprint(\"Best accuracy:\", study.best_value)\n\n# Train the best model\nbest_params = study.best_params\nbest_model = CatBoostClassifier(**best_params, random_seed=42, verbose=0)\nbest_model.fit(X_train, y_train)\n\n# Make predictions\ncatboost_predictions = best_model.predict(X_valid)\n\n# Calculate accuracy\ncatboost_accuracy = accuracy_score(y_valid, catboost_predictions)\nprint(\"\\nCatBoost Accuracy with Optimized Parameters:\", catboost_accuracy)\n\n# Create and display confusion matrix\ncm = confusion_matrix(y_valid, xgb_predictions)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix - Optimized CatBoost\")\nplt.show()\n\n# Print classification report\nreport = classification_report(y_valid, catboost_predictions)\nprint(\"\\nClassification Report:\")\nprint(report)\n\n# Feature importance\nfeature_importance = best_model.feature_importances_\nfor i, importance in enumerate(feature_importance):\n    print(f\"Feature {i}: {importance}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DecisionTreeClassifier","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    # Define hyperparameter search space for Decision Tree\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 20),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2', None]),\n        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n        'min_weight_fraction_leaf': trial.suggest_float('min_weight_fraction_leaf', 0.0, 0.5),\n        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 1000),\n        'random_state': 42\n    }\n    \n    model = DecisionTreeClassifier(**params)\n    model.fit(X_train, y_train)\n    \n    predictions = model.predict(X_valid)\n    accuracy = accuracy_score(y_valid, predictions)\n    \n    return accuracy\n\n# Create and run the optimization study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)  # You can adjust n_trials\n\nprint(\"Best parameters:\", study.best_params)\nprint(\"Best accuracy:\", study.best_value)\n\n# Train the best model\nbest_params = study.best_params\nbest_model = DecisionTreeClassifier(**best_params, random_state=42)\nbest_model.fit(X_train, y_train)\n\n# Make predictions\ndt_predictions = best_model.predict(X_valid)\n\n# Calculate accuracy\ndt_accuracy = accuracy_score(y_valid, dt_predictions)\nprint(\"\\nDecision Tree Accuracy with Optimized Parameters:\", dt_accuracy)\n\n# Create and display confusion matrix\ncm = confusion_matrix(y_valid, dt_predictions)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix - Optimized Decision Tree\")\nplt.show()\n\n# Print classification report\nreport = classification_report(y_valid, dt_predictions)\nprint(\"\\nClassification Report:\")\nprint(report)\n\n# Feature importance\nfeature_importance = best_model.feature_importances_\nfor i, importance in enumerate(feature_importance):\n    print(f\"Feature {i}: {importance}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RandomForestClassifier","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n        'max_depth': trial.suggest_int('max_depth', 3, 20),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2', None]),\n        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n        'random_state': 42\n    }\n    \n    model = RandomForestClassifier(**params)\n    model.fit(X_train, y_train)\n    \n    predictions = model.predict(X_valid)\n    accuracy = accuracy_score(y_valid, predictions)\n    \n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)  # You can adjust n_trials\n\nprint(\"Best parameters:\", study.best_params)\nprint(\"Best accuracy:\", study.best_value)\n\nbest_params = study.best_params\nbest_model = RandomForestClassifier(**best_params, random_state=42)\nbest_model.fit(X_train, y_train)\n\nrf_predictions = best_model.predict(X_valid)\n\nrf_accuracy = accuracy_score(y_valid, rf_predictions)\nprint(\"\\nRandom Forest Accuracy with Optimized Parameters:\", rf_accuracy)\n\ncm = confusion_matrix(y_valid, rf_predictions)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix - Optimized Random Forest\")\nplt.show()\n\nreport = classification_report(y_valid, rf_predictions)\nprint(\"\\nClassification Report:\")\nprint(report)\n\nfeature_importance = best_model.feature_importances_\nfor i, importance in enumerate(feature_importance):\n    print(f\"Feature {i}: {importance}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extra Trees","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n        'max_depth': trial.suggest_int('max_depth', 3, 20),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2', None]),\n        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n        'random_state': 42\n    }\n    \n    model = ExtraTreesClassifier(**params)\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_valid)\n    accuracy = accuracy_score(y_valid, predictions)\n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\n\nprint(\"Best parameters:\", study.best_params)\nprint(\"Best accuracy:\", study.best_value)\n\nbest_params = study.best_params\nbest_model = ExtraTreesClassifier(**best_params, random_state=42)\nbest_model.fit(X_train, y_train)\n\net_predictions = best_model.predict(X_valid)\n\n\nvalid_accuracy = accuracy_score(y_valid, et_predictions)\nprint(\"\\nValidation Accuracy:\", valid_accuracy)\nprint(\"\\nClassification Report on Validation Set:\")\nprint(classification_report(y_valid, y_valid_pred))\n\ny_test_pred = best_model.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprint(\"\\nTest Accuracy:\", test_accuracy)\nprint(\"\\nClassification Report on Test Set:\")\nprint(classification_report(y_test, y_test_pred))\n\net_accuracy = accuracy_score(y_valid, et_predictions)\nprint(\"\\nExtra Trees Accuracy with Optimized Parameters:\", et_accuracy)\n\ncm = confusion_matrix(y_valid, et_predictions)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix - Optimized Extra Trees\")\nplt.show()\n\n# Classification report yazdır\nreport = classification_report(y_valid, et_predictions)\nprint(\"\\nClassification Report:\")\nprint(report)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(y_valid, y_valid_pred, y_test, y_test_pred, model, X_valid):\n    print(\"\\nValidation Accuracy:\", accuracy_score(y_valid, y_valid_pred))\n    print(\"\\nClassification Report on Validation Set:\")\n    print(classification_report(y_valid, y_valid_pred))\n\n    print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_test_pred))\n    print(\"\\nClassification Report on Test Set:\")\n    print(classification_report(y_test, y_test_pred))\n\n    # **Confusion Matrix**\n    cm = confusion_matrix(y_valid, y_valid_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(cmap=plt.cm.Blues)\n    plt.title(f\"Confusion Matrix\")\n    plt.show()\n\n    # **ROC Curve**\n    try:\n        if hasattr(model, \"predict_proba\"):  \n            y_valid_proba = model.predict_proba(X_valid)\n        elif hasattr(model, \"decision_function\"):  \n            y_valid_proba = model.decision_function(X_valid)\n        else:\n            print(\"⚠️ Model does not support probability predictions, skipping ROC Curve.\")\n            return\n\n        # **Binary Classification (Tek Sınıf)**\n        if len(np.unique(y_valid)) == 2:\n            fpr, tpr, _ = roc_curve(y_valid, y_valid_proba[:, 1])\n            roc_auc = auc(fpr, tpr)\n        # **Multi-Class (OVR Strategy)**\n        else:\n            roc_auc = roc_auc_score(y_valid, y_valid_proba, multi_class=\"ovr\")\n            print(\"\\nMulti-Class ROC AUC Score:\", roc_auc)\n            return  \n\n        plt.figure()\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver Operating Characteristic (ROC) Curve')\n        plt.legend(loc=\"lower right\")\n        plt.show()\n\n    except Exception as e:\n        print(f\"❌ Error while generating ROC Curve: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## HistGradientBoostingClassifier","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.5),\n        'max_iter': trial.suggest_int('max_iter', 50, 500),\n        'max_depth': trial.suggest_int('max_depth', 3, 20),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)\n    }\n\n    model = HistGradientBoostingClassifier(**params)\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_valid)\n    accuracy = accuracy_score(y_valid, predictions)\n    \n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\n\nbest_params = study.best_params\nbest_model = HistGradientBoostingClassifier(**best_params)\nbest_model.fit(X_train, y_train)\n\ny_valid_pred = best_model.predict(X_valid)\ny_test_pred = best_model.predict(X_test)\n\nevaluate_model(y_valid, y_valid_pred, y_test, y_test_pred, best_model, X_valid)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LGBMClassifier","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.5),\n        'max_iter': trial.suggest_int('max_iter', 50, 500),\n        'max_depth': trial.suggest_int('max_depth', 3, 20),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)\n    }\n\n    model = LGBMClassifier(**params)\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_valid)\n    accuracy = accuracy_score(y_valid, predictions)\n    \n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\n\nbest_params = study.best_params\nbest_model = LGBMClassifier(**best_params)\nbest_model.fit(X_train, y_train)\n\ny_valid_pred = best_model.predict(X_valid)\ny_test_pred = best_model.predict(X_test)\n\nevaluate_model(y_valid, y_valid_pred, y_test, y_test_pred, best_model, X_valid)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n    }\n\n    model = AdaBoostClassifier(**params)\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_valid)\n    accuracy = accuracy_score(y_valid, predictions)\n    \n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=20)\n\nbest_params = study.best_params\nbest_model = AdaBoostClassifier(**best_params)\nbest_model.fit(X_train, y_train)\n\ny_valid_pred = best_model.predict(X_valid)\ny_test_pred = best_model.predict(X_test)\n\nevaluate_model(y_valid, y_valid_pred, y_test, y_test_pred, best_model, X_valid)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}